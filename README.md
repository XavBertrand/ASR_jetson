# Lightweight ASR Pipeline with Diarization

This repository provides a **lightweight, modular, and efficient Automatic Speech Recognition (ASR) pipeline** designed to run locally on both desktop GPUs and edge devices such as the **Jetson Orin Nano**.  
It combines noise suppression, Voice Activity Detection (VAD), speaker diarization, and ASR transcription in a single, end-to-end workflow.

---

## âœ¨ Features

- ğŸ§ **Noise suppression** with [RNNoise](https://github.com/xiph/rnnoise) to enhance speech quality.  
- ğŸ™ **Voice Activity Detection (VAD)** using [Silero VAD](https://github.com/snakers4/silero-vad) for accurate speech segmentation.  
- ğŸ‘¥ **Speaker diarization** with **TitaNet-S embeddings** and **spectral clustering**, enabling speaker-attributed transcriptions.  
- ğŸ“ **Automatic Speech Recognition (ASR)** using [FasterWhisper](https://github.com/SYSTRAN/faster-whisper) or **NVIDIA FastConformer** (via [NeMo](https://github.com/NVIDIA/NeMo)) for fast and accurate transcription.  
- ğŸ“¦ **Lightweight & modular**: optimized for local use, Jetson deployment, or integration into existing apps.  
- ğŸ§ª **Tested** with unit tests and integration tests (pytest).  

---

## ğŸ›  Pipeline Overview

1. **Noise Suppression**  
   Input audio is denoised with RNNoise to reduce background noise.  

2. **Voice Activity Detection (VAD)**  
   Silero VAD splits audio into speech / non-speech regions.  

3. **Speaker Embedding & Clustering**  
   - Each speech segment is processed with **TitaNet-S** to extract speaker embeddings.  
   - Segments are grouped using **spectral clustering** â†’ speaker diarization.  

4. **ASR Transcription**  
   - Speech segments are transcribed using **FasterWhisper** (Whisper accelerated with CTranslate2) or **NeMo FastConformer**.  
   - Output includes **timestamps, text, and speaker labels**.  

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€run_asr_pipeline.py # Wrapper for full pipeline execution
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile           # Multi arch (x86_64 and arm64) docker file
â”‚   â””â”€â”€ requirements.txt     # python packages requirements
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/      # RNNoise wrapper
â”‚   â”œâ”€â”€ vad/                # Silero VAD integration
â”‚   â”œâ”€â”€ diarization/        # TitaNet-S embeddings + clustering
â”‚   â”œâ”€â”€ asr/                # FasterWhisper / FastConformer ASR
â”‚   â”œâ”€â”€ postprocessing/     # Text export functions
â”‚   â”œâ”€â”€ pipeline/           # End-to-end pipeline orchestration
â”‚   â””â”€â”€ utils/              # Helper functions
â”‚
â”œâ”€â”€ tests/                  # Unit & integration tests (pytest)
â”‚   â”œâ”€â”€ test_full_pipeline.py
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ data/               # Test audio files
â”‚
â”œâ”€â”€ models/                 # Some of the light AI models
â”‚   â”œâ”€â”€ nemo/               # TitaNet-S weights
â”‚   â”œâ”€â”€ rnnoise/            # RNNoise weigths
â”‚
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md               # Project documentation

```

---

## ğŸš€ Installation

### Prerequisites
- Python 3.9+
- CUDA-enabled GPU (for faster inference, optional but recommended)
- [ffmpeg](https://ffmpeg.org/) installed and in PATH

### Setup
```bash
git clone https://github.com/yourusername/asr-pipeline.git
cd asr-pipeline
python -m venv .venv
source .venv/bin/activate  # (Linux/Mac)
.venv\Scripts\activate     # (Windows)
pip install -r requirements.txt
```

## Build multi-arch Docker image

Linux / macOS / WSL:
```bash
./docker/build.sh
```
Windows Powershell:
```bash
.\docker\build.ps1
```

## â–¶ï¸ Usage

### Run from CLI
```bash
python -m src.pipeline --audio_file path/to/file.wav --output transcript.json
```

### Run with Streamlit UI
```bash
streamlit run streamlit_app.py
```

### Example Output
```json
[
  {
    "speaker": "SPEAKER_1",
    "start": 0.5,
    "end": 3.2,
    "text": "Hello everyone, thanks for joining the meeting today."
  },
  {
    "speaker": "SPEAKER_2",
    "start": 3.3,
    "end": 5.7,
    "text": "Good morning, let's get started."
  }
]
```

---

## âœ… Testing

Integration tests ensure the pipeline works end-to-end.  

Run all tests:
```bash
pytest tests
```

---

## ğŸ“Š Benchmarks

| Model              | Device              | 1h audio runtime |
|--------------------|---------------------|------------------|
| Whisper Large      | Desktop GPU (4070) | ~12 min          |
| FasterWhisper-M    | Jetson Orin Nano    | ~25â€“30 min       |
| FastConformer-CTC  | Jetson Orin Nano    | ~20â€“25 min       |

*(Values are indicative and depend on audio quality & hardware setup)*

---

## ğŸ“Œ Roadmap

- [ ] Add support for **online streaming transcription**  
- [ ] Extend diarization with **overlapping speech detection**  
- [ ] Add **speaker adaptation** (personalized profiles)  
- [ ] Docker container for **easy Jetson deployment**  

---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [Silero VAD](https://github.com/snakers4/silero-vad)  
- [NVIDIA NeMo](https://github.com/NVIDIA/NeMo)  
- [TitaNet](https://arxiv.org/abs/2110.04410)  
- [Whisper & FasterWhisper](https://github.com/openai/whisper)  
- [RNNoise](https://github.com/xiph/rnnoise)  
