# Lightweight ASR Pipeline with Diarization

This repository provides a **lightweight, modular, and efficient Automatic Speech Recognition (ASR) pipeline** designed to run locally on both desktop GPUs and edge devices such as the **Jetson Orin Nano**.
It combines noise suppression, Voice Activity Detection (VAD), speaker diarization, and ASR transcription in a single, end-to-end workflow.

---

## âœ¨ Features

* ğŸ§ **Noise suppression** with [RNNoise](https://github.com/xiph/rnnoise) to enhance speech quality.
* ğŸ¤ **Voice Activity Detection (VAD)** using [Silero VAD](https://github.com/snakers4/silero-vad) for accurate speech segmentation.
* ğŸ‘¥ **Speaker diarization** with **TitaNet-S embeddings** and **spectral clustering**, enabling speaker-attributed transcriptions.
* ğŸ“ **Automatic Speech Recognition (ASR)** using [FasterWhisper](https://github.com/SYSTRAN/faster-whisper) or **NVIDIA FastConformer** (via [NeMo](https://github.com/NVIDIA/NeMo)).
* âš¡ **Optimized for Jetson Orin Nano**: runs locally with CUDA/TensorRT acceleration.
* ğŸ§± **uv + pyproject.toml** build system (no `requirements.txt` needed).
* ğŸ¥ª Unit and integration tests with pytest.

---

## ğŸ“‚ Repository Structure

```
ASR_jetson/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ asr_jetson/
â”‚       â”œâ”€â”€ preprocessing/        # RNNoise wrapper
â”‚       â”œâ”€â”€ vad/                  # Silero VAD integration
â”‚       â”œâ”€â”€ diarization/          # TitaNet-S embeddings + clustering
â”‚       â”œâ”€â”€ asr/                  # FasterWhisper / NeMo FastConformer
â”‚       â”œâ”€â”€ postprocessing/       # Text cleaning and formatting
â”‚       â”œâ”€â”€ pipeline/             # End-to-end pipeline orchestration (core + CLI)
â”‚       â”œâ”€â”€ io/                   # Audio I/O and storage utilities
â”‚       â””â”€â”€ utils/                # Configs, helpers
â”‚
â”œâ”€â”€ configs/                      # (optional) runtime YAML configs
â”‚   â”œâ”€â”€ dev.yaml
â”‚   â””â”€â”€ jetson.yaml
â”‚
â”œâ”€â”€ tests/                        # Unit & integration tests
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â””â”€â”€ data/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                # Multi-arch (x86_64 + ARM64) build
â”‚   â””â”€â”€ Dockerfile.jetson         # Jetson Orin Nano deployment
â”‚
â”œâ”€â”€ pyproject.toml                # Dependencies, scripts, and settings
â”œâ”€â”€ uv.lock                       # uv dependency lock file
â””â”€â”€ README.md
```

---

## ğŸ›  Installation (with uv)

### Prerequisites

* Python â‰¥ 3.10
* CUDA-enabled GPU (recommended)
* [ffmpeg](https://ffmpeg.org/) in PATH
* [uv](https://github.com/astral-sh/uv) installed (`pip install uv`)

### Setup

```bash
git clone https://github.com/XavBertrand/ASR_jetson.git
cd ASR_jetson

# Create virtual environment and install dependencies
uv sync --extra dev --extra media

# (Optional) add GPU support on Windows
uv add "torch==2.4.0+cu124" --extra-index-url https://download.pytorch.org/whl/cu124
```

---

## â–¶ï¸ Usage

### Run from CLI

```bash
uv run asr-pipeline --audio path/to/file.wav --out out/transcript.json
```

Or directly:

```bash
uv run python -m asr_jetson --audio path/to/file.wav
```

### Example Output

```json
[
  {
    "speaker": "SPEAKER_1",
    "start": 0.5,
    "end": 3.2,
    "text": "Hello everyone, thanks for joining the meeting today."
  },
  {
    "speaker": "SPEAKER_2",
    "start": 3.3,
    "end": 5.7,
    "text": "Good morning, let's get started."
  }
]
```

---

## ğŸ³ Docker

### ğŸ§¹ Local / Desktop build (x86_64)

```bash
docker build -t asr-jetson:dev -f docker/Dockerfile .
```

### ğŸš€ Jetson Orin Nano build

Uses NVIDIAâ€™s `l4t-ml` base (includes CUDA + PyTorch).
Make sure JetPack â‰¥ 6.0.

```bash
docker build -t asr-jetson:jetson -f docker/Dockerfile.jetson .
```

**Key points:**

* No more `requirements.txt` â€” dependencies are installed via `uv sync` using `pyproject.toml`.
* Torch is already included in `l4t-ml`.
* Volumes can be mounted for I/O:

  ```bash
  docker run --gpus all -v $(pwd)/data:/data -v $(pwd)/models:/models -v $(pwd)/output:/output asr-jetson:jetson
  ```

### ğŸ”± Multi-arch build (x86_64 + ARM64)

```bash
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  -t xavbertrand/asr-jetson:latest \
  -f docker/Dockerfile.jetson \
  --push .
```

---

## âœ… Testing

```bash
uv run pytest
```

To skip GPU tests on CPU:

```bash
pytest -m "not gpu"
```

---

## ğŸ“Š Benchmarks

| Model             | Device             | 1h audio runtime |
| ----------------- | ------------------ | ---------------- |
| Whisper Large     | Desktop GPU (4070) | ~12 min          |
| FasterWhisper-M   | Jetson Orin Nano   | ~25â€“30 min       |
| FastConformer-CTC | Jetson Orin Nano   | ~20â€“25 min       |

*(Approximate values depending on model and precision settings)*

---

## ğŸ–Š Roadmap

* [ ] Add support for **online streaming transcription**
* [ ] Integrate **FastAPI service** for remote inference
* [ ] Add **speaker adaptation** (personalized profiles)
* [ ] Extend diarization with **overlap detection**

---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

* [Silero VAD](https://github.com/snakers4/silero-vad)
* [NVIDIA NeMo](https://github.com/NVIDIA/NeMo)
* [TitaNet](https://arxiv.org/abs/2110.04410)
* [Whisper & FasterWhisper](https://github.com/openai/whisper)
* [RNNoise](https://github.com/xiph/rnnoise)
* [uv](https://github.com/astral-sh/uv)
