# src/postprocessing/llm_clean.py
from __future__ import annotations
import os
from pathlib import Path
import json
import re
import time
import requests
from typing import Optional

os.environ["LLM_ENDPOINT"] = "http://tensorrt-llm:8000"
os.environ["LLM_MODEL"] = "gemma2:2b"

SYS_PROMPT = (
    "Tu es un correcteur automatique de transcription fran√ßaise. "
    "R√®gles: 1) Corrige orthographe, grammaire, accords, ponctuation fran√ßaise (espaces fines avant ; : ! ? facultatives, mais espace normal OK), "
    "2) Garde le sens AU PLUS PROCHE possible, 3) Ne SUPPRIME pas d'information s√©mantique, "
    "4) Ne r√©invente rien, 5) Conserve la structure et les tags de locuteur 'SPEAKER_X:' "
    "6) Fusionne les micro-b√©gaiements √©vidents, 7) Ne change pas les num√©ros de SPEAKER."
    "8) Tu n‚Äô√©cris JAMAIS de commentaires ni d‚Äôanalyses. "
    "9) Tu produis UNIQUEMENT le texte corrig√©, sans aucun ajout ni retrait. "
    "10) Ton r√¥le est m√©canique, comme un correcteur orthographique humain disciplin√©."
)

FEW_SHOT_EXAMPLES = """
EXEMPLE 1:
Entr√©e:
SPEAKER_1: bonjour je mappel jean je suis content detre ici
SPEAKER_2: enchant√© moi cest marie

Sortie:
SPEAKER_1: Bonjour, je m'appelle Jean. Je suis content d'√™tre ici.
SPEAKER_2: Enchant√©, moi c'est Marie.

EXEMPLE 2:
Entr√©e:
SPEAKER_1: decleration des droit de lhomme et du citoyen
SPEAKER_2: oui cest un texte fondementale

Sortie:
SPEAKER_1: D√©claration des droits de l'homme et du citoyen.
SPEAKER_2: Oui, c'est un texte fondamental.
"""

USER_INSTR = (
    "IMPORTANT : Ce qui suit N'EST PAS une analyse, pas un r√©sum√©, pas une reformulation.\n"
    "Tu n'es PAS critique litt√©raire, tu es un correcteur automatique.\n"
    "TA SEULE MISSION : corriger les fautes d‚Äôorthographe, de grammaire et de ponctuation du texte donn√©, "
    "sans rien changer d‚Äôautre.\n"
    "R√®gles obligatoires :\n"
    "1Ô∏è) Ne supprime RIEN.\n"
    "2Ô∏è) Ne r√©sume RIEN.\n"
    "3Ô∏è) Ne commente RIEN.\n"
    "4Ô∏è) Ne r√©√©cris PAS le style.\n"
    "5Ô∏è) Ne traduis PAS.\n"
    "6Ô∏è) Tu dois garder le m√™me format exact que le texte d‚Äôentr√©e : m√™mes lignes, m√™mes pr√©fixes 'SPEAKER_X:'.\n"
    "7Ô∏è) Si tu sors autre chose que le texte corrig√©, la sortie sera REJET√âE.\n"
    "Format attendu (exemple) :\n"
    "Entr√©e : SPEAKER_1: bonjour je suis alle a la plage\n"
    "Sortie : SPEAKER_1: Bonjour, je suis all√© √† la plage.\n"
    "Ne r√©ponds QUE par la version corrig√©e, sans introduction, sans explication.\n"
    + FEW_SHOT_EXAMPLES +
    "MAINTENANT TON TOUR, voici le texte √† corriger :\n"
)

def _basic_local_cleanup(text: str) -> str:
    """Fallback minimal si aucun LLM dispo (s√©curise la pipeline)."""
    import re

    # 1. R√©duction basique des espaces
    s = re.sub(r"\s+", " ", text).strip()

    # 2. Ponctuation : espace avant ‚Üí retirer, espace apr√®s ‚Üí forcer
    s = re.sub(r"\s+([,.;:!?])", r"\1", s)
    s = re.sub(r"([,;:!?])(?=\S)", r"\1 ", s)

    # 3. Apostrophes typographiques
    s = s.replace(" '", " ‚Äô").replace("' ", "‚Äô ").replace("'", "‚Äô")

    # 4. Majuscule en d√©but de phrase
    s = re.sub(r"(^|[.!?]\s+)(\w)", lambda m: m.group(1) + m.group(2).upper(), s)

    # 5. SPEAKER tags sur nouvelle ligne + **un seul espace apr√®s**
    s = re.sub(r"\s*(SPEAKER_\d+:)\s*", r"\n\1 ", s).strip()

    # 6. Nettoyage des doublons de retours √† la ligne
    s = re.sub(r"\n+", "\n", s)

    # 7. Fin de ligne finale
    if not s.endswith("\n"):
        s += "\n"

    return s


def _call_openai_compatible(endpoint: str, api_key: Optional[str], model: str, text: str, timeout_s: int = 120) -> Optional[str]:
    """
    Appelle une API 'OpenAI-compatible' (TensorRT-LLM engine server, vLLM, LM Studio, OpenRouter local, etc.)
    """
    url = endpoint.rstrip("/") + "/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYS_PROMPT},
            {"role": "user", "content": USER_INSTR + "\n\n" + text},
        ],
        # "temperature": 0.1,
        # "max_tokens": max(512, min(4096, len(text.split()) * 2)),
        "temperature": 0.0,
        "max_tokens": len(text.split()) * 3,
        "stream": False,
    }
    r = requests.post(url, headers=headers, json=payload, timeout=timeout_s)
    if r.status_code == 200:
        data = r.json()
        return data["choices"][0]["message"]["content"].strip()
    return None

# def _call_ollama(model: str, text: str, timeout_s: int = 120) -> Optional[str]:
#     """
#     Appelle un serveur Ollama local (http://localhost:11434) si dispo.
#     """
#     url = "http://localhost:11434/api/chat"
#     payload = {
#         "model": model,
#         "messages": [
#             {"role": "system", "content": SYS_PROMPT},
#             {"role": "user", "content": USER_INSTR + "\n\n" + text},
#         ],
#         "options": {"temperature": 0.1},
#         "stream": False,
#     }
#     r = requests.post(url, json=payload, timeout=timeout_s)
#     if r.status_code == 200:
#         return r.json().get("message", {}).get("content", "").strip()
#     return None

def _call_ollama(model: str, text: str, timeout_s: int = 120) -> Optional[str]:
    """
    Appelle Ollama /api/chat proprement pour Gemma 2.
    - Pas d'amor√ßage 'assistant' en dernier (√ßa casse le template de certains mod√®les).
    - Augmente num_ctx pour √©viter la troncature des longs textes.
    - √âvite les few-shots si l'entr√©e est d√©j√† longue.
    """
    import re
    url = "http://localhost:11434/api/chat"

    # Si le texte est long, on retire les EXEMPLES du prompt (ils bouffent du contexte)
    user_msg = USER_INSTR + "\n\n" + text
    if len(text.split()) > 1200:  # seuil empirique
        user_msg = USER_INSTR.replace(FEW_SHOT_EXAMPLES, "") + "\n\n" + text

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYS_PROMPT.replace("JA%AIS", "JAMAIS")},
            {"role": "user",   "content": user_msg},
            # Ne PAS ajouter de message assistant ici
        ],
        "options": {
            "temperature": 0.0,
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "num_predict": max(512, len(text.split()) * 2),
            "num_ctx": 8192  # <-- IMPORTANT : augmente la fen√™tre de contexte si ton mod√®le le supporte
        },
        "stream": False,
    }

    try:
        r = requests.post(url, json=payload, timeout=timeout_s)
        if r.status_code == 200:
            content = r.json().get("message", {}).get("content", "").strip()
            # Sanity check : si le mod√®le n'a pas vu le texte, il r√©pond parfois "Commencez...", "J'attends..."
            lower = content.lower()
            if ("commencez" in lower or "j'attends votre transcription" in lower
                or ("corriger" in lower and len(content.split()) < 12)):
                # On r√©essaie en 'g√©n√©ration simple' avec amorce stricte
                return _ollama_generate_prefix(model, text, timeout_s)
            # Validation existante (tags SPEAKER, longueur, etc.)
            if _validate_ollama_output(text, content):
                return content
        else:
            print(f"‚ö†Ô∏è Ollama HTTP {r.status_code}: {r.text[:200]}")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur Ollama: {e}")
    return None

def _ollama_generate_prefix(model: str, text: str, timeout_s: int = 120) -> Optional[str]:
    """Fallback : /api/generate avec amorce stricte 'SPEAKER_X: ' pour forcer le format."""
    import re, json, requests
    url = "http://localhost:11434/api/generate"
    first = re.search(r'(SPEAKER_\d+:)', text)
    prefix = (first.group(1) + " ") if first else "SPEAKER_1: "
    prompt = (
        SYS_PROMPT.replace("JA%AIS", "JAMAIS")
        + "\n\n"
        + USER_INSTR.replace(FEW_SHOT_EXAMPLES, "")
        + "\n\n"
        + "R√©ponds en commen√ßant exactement par: " + prefix + "\n\n"
        + text
        + "\n\n"
    )
    payload = {
        "model": model,
        "prompt": prompt,
        "options": {"temperature": 0.0, "num_predict": max(512, len(text.split()) * 2), "num_ctx": 8192},
        "stream": False,
    }
    r = requests.post(url, json=payload, timeout=timeout_s)
    if r.status_code == 200:
        out = r.json().get("response", "").strip()
        # Pr√©fixe garanti
        if not out.startswith(prefix):
            out = prefix + out
        return out if _validate_ollama_output(text, out) else None
    return None


def _validate_ollama_output(original: str, corrected: str) -> bool:
    """Valide que Ollama a bien corrig√© et non analys√©."""

    # 1. V√©rifier les SPEAKER tags
    orig_speakers = set(re.findall(r'SPEAKER_\d+:', original))
    corr_speakers = set(re.findall(r'SPEAKER_\d+:', corrected))
    if orig_speakers != corr_speakers:
        print(f"‚ö†Ô∏è SPEAKER tags diff√©rents: {orig_speakers} vs {corr_speakers}")
        return False

    # 2. V√©rifier la longueur (doit rester similaire ¬±50%)
    orig_words = len(original.split())
    corr_words = len(corrected.split())
    if corr_words < orig_words * 0.5 or corr_words > orig_words * 1.5:
        print(f"‚ö†Ô∏è Longueur suspecte: {orig_words} mots ‚Üí {corr_words} mots")
        return False

    # 3. D√©tecter si c'est une analyse (mots interdits au d√©but)
    forbidden_start = ["voici", "cette", "le texte", "analyse", "r√©sum√©", "th√®me"]
    first_line = corrected.split('\n')[0].lower()
    if any(first_line.startswith(word) for word in forbidden_start):
        print(f"‚ö†Ô∏è D√©tection d'analyse: {first_line[:100]}")
        return False

    # 4. V√©rifier qu'il n'y a pas de structure d'analyse (num√©ros, tirets)
    if re.search(r'^\d+\.', corrected, re.MULTILINE) or corrected.count('-') > orig_words * 0.1:
        print(f"‚ö†Ô∏è Structure d'analyse d√©tect√©e")
        return False

    return True

def _validate_structure(original: str, corrected: str) -> bool:
    """V√©rifie qu'on a conserv√© les m√™mes SPEAKER tags et la m√™me structure de lignes."""
    import re
    orig_tags = re.findall(r'^(\s*SPEAKER_\d+:\s*)', original, flags=re.MULTILINE)
    corr_tags = re.findall(r'^(\s*SPEAKER_\d+:\s*)', corrected, flags=re.MULTILINE)
    return orig_tags == corr_tags and set(re.findall(r'SPEAKER_\d+:', original)) == set(re.findall(r'SPEAKER_\d+:', corrected))

def _clean_with_languagetool(text: str) -> Optional[str]:
    """
    Fallback 'l√©ger' via LanguageTool (rule-based) ‚Äî sans LLM.
    - Corrige ligne par ligne pour pr√©server strictement les pr√©fixes 'SPEAKER_X:'.
    - Si LT n'est pas dispo (module/Java), renvoie None.
    ENV optionnel: LT_ENDPOINT=http://host:port (serveur LT distant)
    """
    try:
        import os, re
        import language_tool_python

        lt_endpoint = os.getenv("LT_ENDPOINT", "").strip() or None
        if lt_endpoint:
            tool = language_tool_python.LanguageTool('fr', remote_server=lt_endpoint)
        else:
            tool = language_tool_python.LanguageTool('fr')  # n√©cessite Java local

        out_lines = []
        for line in text.splitlines():
            m = re.match(r'^(\s*SPEAKER_\d+:\s*)(.*)$', line)
            if m:
                prefix, content = m.group(1), m.group(2)
                # Correction uniquement sur le contenu, jamais sur le pr√©fixe
                corrected = tool.correct(content)
                # Normalise un seul espace apr√®s "SPEAKER_X:"
                out_lines.append(f"{prefix.strip()} {corrected.strip()}")
            else:
                # Hors ligne SPEAKER, corrige tel quel (ou laisse brut si tu pr√©f√®res)
                out_lines.append(tool.correct(line))

        corrected_text = "\n".join(out_lines).rstrip() + "\n"
        # Garde-fou : m√™me structure de tags et ordre des lignes SPEAKER
        if _validate_structure(text, corrected_text):
            return corrected_text
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è LanguageTool indisponible: {e}")
        return None


def clean_text_with_llm(
    input_txt: Path,
    output_txt: Path,
    default_model: str = "gemma2:2b",
) -> Path:
    input_txt = Path(input_txt)
    output_txt = Path(output_txt)
    output_txt.parent.mkdir(parents=True, exist_ok=True)

    raw = input_txt.read_text(encoding="utf-8")

    # 1) OpenAI-compatible (garde ton code existant)
    endpoint = os.getenv("LLM_ENDPOINT", "").strip()
    if endpoint:
        model = os.getenv("LLM_MODEL", default_model).strip() or default_model
        api_key = os.getenv("LLM_API_KEY", "").strip() or None
        try:
            out = _call_openai_compatible(endpoint, api_key, model, raw)
            if out and _validate_ollama_output(raw, out):
                output_txt.write_text(out + "\n", encoding="utf-8")
                print("‚úÖ Correction LLM (OpenAI-compatible) OK")
                return output_txt
            else:
                print("‚ö†Ô∏è Sortie OpenAI-compatible invalide")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur OpenAI-compatible: {e}")

    # 2) Ollama local (VERSION AM√âLIOR√âE)
    ollama_model = os.getenv("OLLAMA_MODEL", "").strip()
    if ollama_model:
        print(f"üîÑ Tentative Ollama avec mod√®le: {ollama_model}")
        try:
            out = _call_ollama(ollama_model, raw)
            if out:
                output_txt.write_text(out + "\n", encoding="utf-8")
                print("‚úÖ Correction Ollama OK")
                return output_txt
            else:
                print("‚ö†Ô∏è Sortie Ollama invalide, fallback regex")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur Ollama: {e}")

    # 3) Fallback LanguageTool (rule-based) si LLM KO
    print("üîÑ Tentative LanguageTool (fallback non-LLM)")
    lt_out = _clean_with_languagetool(raw)
    if lt_out:
        output_txt.write_text(lt_out, encoding="utf-8")
        print("‚úÖ Correction LanguageTool OK")
        return output_txt
    else:
        print("‚ö†Ô∏è LanguageTool indisponible ou structure invalide, fallback regex")

    # 4) Fallback regex minimal (toujours fonctionner)
    print("üîß Utilisation du fallback regex")
    cleaned = _basic_local_cleanup(raw)
    output_txt.write_text(cleaned + "\n", encoding="utf-8")
    return output_txt
