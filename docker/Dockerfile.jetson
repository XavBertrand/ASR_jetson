# =============================================================================
# Dockerfile Jetson (arm64) — ASR pipeline
# Base NVIDIA L4T-ML (PyTorch + CUDA + cuDNN + TensorRT déjà intégrés)
# =============================================================================
ARG L4T_TAG=r36.2.0-py3
FROM nvcr.io/nvidia/l4t-ml:${L4T_TAG} AS base

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_NO_CACHE_DIR=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    TZ=Etc/UTC \
    OMP_NUM_THREADS=1

# L4T-ML contient déjà la plupart des outils nécessaires
# On installe uniquement ce qui manque vraiment
RUN apt-get update && apt-get install -y --no-install-recommends \
      curl \
      tini \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean || true

# -----------------------------------------------------------------------------
# STAGE: builder — création du venv et installation des dépendances
# -----------------------------------------------------------------------------
FROM base AS builder
WORKDIR /build

# Installation de uv (gestionnaire de packages moderne)
RUN curl -LsSf https://astral.sh/uv/install.sh | sh

# Mise à jour du PATH pour inclure uv
ENV PATH="/root/.local/bin:$PATH"

# Création du venv dans /opt/venv
RUN uv venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH" \
    VIRTUAL_ENV="/opt/venv"

# Copie uniquement pyproject.toml pour maximiser le cache Docker
COPY pyproject.toml ./

# Installation des dépendances
# - media: PyAV pour traitement audio/vidéo
# - tensorrt: TensorRT et PyCUDA (Jetson uniquement)
# - nemo: NeMo ASR toolkit
RUN uv pip install --no-cache \
    -e ".[media,tensorrt,nemo]" \
    && uv pip list

# Vérification de l'installation (échoue le build si problème)
RUN python3 -c "import sys; print('Python:', sys.version)" && \
    python3 -c "import torch; print('PyTorch:', torch.__version__, '| CUDA:', torch.cuda.is_available())" && \
    python3 -c "import nemo; print('NeMo:', nemo.__version__)" && \
    python3 -c "import transformers; print('Transformers:', transformers.__version__)" && \
    python3 -c "import huggingface_hub; print('HF Hub:', huggingface_hub.__version__)"

# -----------------------------------------------------------------------------
# STAGE: runtime — image finale optimisée
# -----------------------------------------------------------------------------
FROM base AS runtime
WORKDIR /app

# Création utilisateur non-root pour la sécurité
RUN useradd -u 1000 -ms /bin/bash appuser && \
    chown -R appuser:appuser /app

# Copie du venv complet depuis le builder
COPY --from=builder /opt/venv /opt/venv

# Configuration PATH pour utiliser le venv
ENV PATH="/opt/venv/bin:$PATH" \
    VIRTUAL_ENV="/opt/venv"

# Copie du code source de l'application
COPY --chown=appuser:appuser ./src ./src
COPY --chown=appuser:appuser ./models ./models
COPY --chown=appuser:appuser ./scripts ./scripts
COPY --chown=appuser:appuser pyproject.toml ./

# Variables d'environnement runtime
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    PYTHONPATH=/app \
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 \
    HF_HUB_DISABLE_TELEMETRY=1 \
    HF_HOME=/app/.cache/huggingface \
    TORCH_HOME=/app/.cache/torch

# Création des répertoires de cache
RUN mkdir -p /app/.cache/huggingface /app/.cache/torch && \
    chown -R appuser:appuser /app/.cache

# Passage en utilisateur non-root
USER appuser

# Port exposé (si vous avez une API)
EXPOSE 8000

# Health check pour orchestration (Kubernetes, Docker Swarm, etc.)
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD curl -fsS http://localhost:8000/health || exit 1

# Tini comme PID 1 pour gérer proprement les signaux
ENTRYPOINT ["/usr/bin/tini", "--"]

# Commande par défaut — à adapter selon votre point d'entrée
CMD ["python3", "src/pipeline/full_pipeline.py"]