# DÃ©ploiement TensorRT-LLM sur Jetson Orin Nano

Guide complet pour dÃ©ployer Qwen 2.5 1.5B avec TensorRT-LLM sur votre Jetson.

## ğŸ“‹ PrÃ©requis

- Jetson Orin Nano avec JetPack 6.0+ (r36.2.0)
- Au moins 8 GB RAM disponible
- Au moins 20 GB d'espace disque libre
- Docker et nvidia-container-runtime installÃ©s

## ğŸš€ DÃ©ploiement rapide (recommandÃ©)

### Option A : Avec moteur prÃ©-buildÃ©

Si vous avez dÃ©jÃ  un moteur TensorRT-LLM :

```bash
# 1. Cloner le repo
cd ~/ASR_Agent

# 2. CrÃ©er les rÃ©pertoires pour les moteurs
mkdir -p volumes/trtllm-engines volumes/trtllm-checkpoints

# 3. Copier votre moteur prÃ©-buildÃ© (si disponible)
# cp -r /path/to/qwen2.5-1.5b-engine volumes/trtllm-engines/qwen2.5-1.5b

# 4. Lancer les services
docker-compose -f docker-compose.jetson.yml up -d

# 5. VÃ©rifier le status
docker-compose -f docker-compose.jetson.yml ps
docker-compose -f docker-compose.jetson.yml logs -f tensorrt-llm
```

### Option B : Build du moteur Ã  la volÃ©e

Si vous n'avez pas encore de moteur :

```bash
# 1. Lancer uniquement le service TensorRT-LLM
docker-compose -f docker-compose.jetson.yml up -d tensorrt-llm

# 2. Entrer dans le container
docker exec -it trtllm-qwen bash

# 3. Builder le moteur (15-30 minutes)
/app/build_engine.sh

# 4. VÃ©rifier que le moteur est crÃ©Ã©
ls -lh /workspace/trt_engines/qwen2.5-1.5b/

# 5. RedÃ©marrer le service
exit
docker-compose -f docker-compose.jetson.yml restart tensorrt-llm

# 6. Lancer le service ASR
docker-compose -f docker-compose.jetson.yml up -d asr-pipeline
```

## ğŸ§ª Tester l'installation

### Test du serveur TensorRT-LLM

```bash
# Health check
curl http://localhost:8001/health

# Test de gÃ©nÃ©ration
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-1.5b-instruct",
    "messages": [
      {
        "role": "system",
        "content": "Tu es un assistant de correction de texte."
      },
      {
        "role": "user",
        "content": "Corrige ce texte: bonjour  ,je  mappelle  xavier"
      }
    ],
    "temperature": 0.1,
    "max_tokens": 100
  }'
```

### Test de votre pipeline ASR

```bash
# Votre pipeline ASR devrait maintenant utiliser automatiquement TensorRT-LLM
# VÃ©rifier les logs
docker-compose -f docker-compose.jetson.yml logs -f asr-pipeline
```

## ğŸ“Š Performances attendues

Sur Jetson Orin Nano (8 GB) :

| MÃ©trique | Valeur |
|----------|--------|
| **Latence (prompt 100 tokens)** | ~200-400ms |
| **Throughput** | ~30-50 tokens/s |
| **RAM utilisÃ©e** | ~2-3 GB |
| **VRAM utilisÃ©e** | ~1.5-2 GB |
| **Temps de build moteur** | 15-30 min |

## ğŸ”§ Configuration avancÃ©e

### Optimiser pour votre cas d'usage

Modifiez `build_qwen_trt_engine.sh` :

```bash
# Pour des textes plus courts (transcriptions courtes)
MAX_INPUT_LEN=512
MAX_OUTPUT_LEN=256

# Pour des textes plus longs (transcriptions longues)
MAX_INPUT_LEN=4096
MAX_OUTPUT_LEN=1024

# Batch size (si vous traitez plusieurs fichiers)
MAX_BATCH_SIZE=8
```

### Variables d'environnement

Dans `docker-compose.jetson.yml`, vous pouvez ajuster :

```yaml
environment:
  - LLM_ENDPOINT=http://tensorrt-llm:8000
  - LLM_MODEL=qwen2.5-1.5b-instruct
  - LLM_API_KEY=  # Optionnel si vous ajoutez de l'auth
```

## ğŸ› DÃ©pannage

### Le moteur ne se build pas

```bash
# VÃ©rifier les logs
docker-compose -f docker-compose.jetson.yml logs tensorrt-llm

# VÃ©rifier l'espace disque
df -h

# VÃ©rifier la RAM disponible
free -h
```

### Erreur "CUDA out of memory"

```bash
# RÃ©duire le batch size
# Dans build_qwen_trt_engine.sh :
MAX_BATCH_SIZE=1
```

### Le serveur ne dÃ©marre pas

```bash
# VÃ©rifier que le moteur existe
docker exec -it trtllm-qwen ls -lh /workspace/trt_engines/qwen2.5-1.5b/

# VÃ©rifier les permissions
docker exec -it trtllm-qwen chmod -R 755 /workspace/trt_engines/
```

## ğŸ“¦ Structure des fichiers

```
~/ASR_Agent/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.jetson            # Votre image ASR
â”‚   â””â”€â”€ Dockerfile.tensorrt-llm      # Image TensorRT-LLM
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_qwen_trt_engine.sh     # Script de build
â”‚   â””â”€â”€ trtllm_server.py             # Serveur API
â”œâ”€â”€ docker-compose.jetson.yml        # Orchestration
â””â”€â”€ volumes/                         # Persistent data
    â”œâ”€â”€ trtllm-engines/              # Moteurs TRT (rebuild pas nÃ©cessaire)
    â””â”€â”€ trtllm-checkpoints/          # Checkpoints intermÃ©diaires
```

## ğŸ”„ Mise Ã  jour

Pour mettre Ã  jour vers une nouvelle version de Qwen :

```bash
# 1. ArrÃªter les services
docker-compose -f docker-compose.jetson.yml down

# 2. Supprimer l'ancien moteur
rm -rf volumes/trtllm-engines/qwen2.5-1.5b/*

# 3. Modifier MODEL_NAME dans build_qwen_trt_engine.sh
# MODEL_NAME="Qwen/Qwen2.5-3B-Instruct"  # Exemple pour 3B

# 4. Rebuild et redÃ©marrer
docker-compose -f docker-compose.jetson.yml up -d --build
```

## ğŸ’¾ Sauvegarde du moteur

Le moteur TensorRT est lourd Ã  rebuilder (15-30 min). Sauvegardez-le :

```bash
# CrÃ©er une archive du moteur
tar -czf qwen2.5-1.5b-trt-engine.tar.gz volumes/trtllm-engines/qwen2.5-1.5b/

# Restaurer sur une autre machine
tar -xzf qwen2.5-1.5b-trt-engine.tar.gz -C volumes/trtllm-engines/
```

## ğŸ“š Ressources

- [TensorRT-LLM Documentation](https://github.com/NVIDIA/TensorRT-LLM)
- [Qwen 2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
- [Jetson AI Lab](https://www.jetson-ai-lab.com/)